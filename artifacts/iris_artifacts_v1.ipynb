{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using W&B Artifacts in your ML Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://www.kaggle.com/chungyehwang/scikit-learn-classifiers-on-iris-dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set any environment variables. If running against a local W&B you should provide Host and Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['WANDB_API_KEY'] = ''\n",
    "#os.environ['WANDB_BASE_URL'] = ''\n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'Iris Artifacts Demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'Base-Run' # Change this as you run new set of runs to see the results for different groups\n",
    "\n",
    "project_name = 'artifacts_demo'\n",
    "\n",
    "general_config = dict(\n",
    "                     tsize = 0.3, # Default = 0.3\n",
    "                     gamma = 0.1, # Default = 0.1\n",
    "                     C = 1.0, # Default = 1.0\n",
    "                     seed = 0 # Default = 0\n",
    "                     ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Dataset and register in Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=project_name, job_type='data_load', group=group_name)\n",
    "\n",
    "artifact = wandb.Artifact('raw_data', type='dataset', metadata={'Source':'https://datahub.io/machine-learning/iris'})\n",
    "\n",
    "artifact.add_file('data/raw/iris_csv.csv')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Raw Dataset from Artifacts, Create Train/Test Split and log back into Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=project_name, job_type='data_prep', group=group_name)\n",
    "\n",
    "wandb.config.test_size = general_config['tsize']\n",
    "wandb.config.seed = general_config['seed']\n",
    "wandb.config.rawdata_artifact = 'raw_data:latest'\n",
    "\n",
    "artifact = run.use_artifact(wandb.config.rawdata_artifact)\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "iris = pd.read_csv(os.path.join(artifact_dir, 'iris_csv.csv'))\n",
    "\n",
    "# Train/Test Split\n",
    "X = iris.iloc[:,0:3]\n",
    "y = iris.iloc[:,4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=wandb.config.test_size, random_state=wandb.config.seed)\n",
    "\n",
    "X_train.to_pickle('data/split/x_train.pkl')\n",
    "X_test.to_pickle('data/split/x_test.pkl')\n",
    "y_train.to_pickle('data/split/y_train.pkl')\n",
    "y_test.to_pickle('data/split/y_test.pkl')\n",
    "\n",
    "artifact = wandb.Artifact('split_data', type='dataset', metadata={'Train Pct':1-wandb.config.test_size, 'Test Pct': wandb.config.test_size})\n",
    "artifact.add_dir('data/split', name='train_test_split')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling and register prepared binaries into Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=project_name, job_type='data_prep', group=group_name)\n",
    "\n",
    "wandb.config.splitdata_artifact = 'split_data:latest'\n",
    "\n",
    "artifact = run.use_artifact(wandb.config.splitdata_artifact)\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "X_train = pd.read_pickle(os.path.join(artifact_dir, 'train_test_split/x_train.pkl'))\n",
    "X_test = pd.read_pickle(os.path.join(artifact_dir, 'train_test_split/x_test.pkl'))\n",
    "y_train = pd.read_pickle(os.path.join(artifact_dir, 'train_test_split/y_train.pkl'))\n",
    "y_test = pd.read_pickle(os.path.join(artifact_dir, 'train_test_split/y_test.pkl'))\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "np.save('data/prepped/x_train_std.npy', X_train_std)\n",
    "np.save('data/prepped/x_test_std.npy', X_test_std)\n",
    "np.save('data/prepped/x_combined_std.npy', X_combined_std)\n",
    "np.save('data/prepped/y_combined.npy', y_combined)\n",
    "\n",
    "artifact = wandb.Artifact('prepped_data', type='dataset')\n",
    "artifact.add_dir('data/prepped')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prepared datasets and train model. Register final model into Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=project_name, job_type='train', group=group_name)\n",
    "\n",
    "wandb.config.seed = general_config['seed']\n",
    "wandb.config.gamma = general_config['gamma']\n",
    "wandb.config.C = general_config['C']\n",
    "wandb.config.preppeddata_artifact = 'prepped_data:latest'\n",
    "\n",
    "artifact = run.use_artifact(wandb.config.preppeddata_artifact)\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "X_train_std = np.load(os.path.join(artifact_dir, 'x_train_std.npy'))\n",
    "X_test_std = np.load(os.path.join(artifact_dir, 'x_test_std.npy'))\n",
    "X_combined_std = np.load(os.path.join(artifact_dir, 'x_combined_std.npy'), allow_pickle=True)\n",
    "y_combined = np.load(os.path.join(artifact_dir, 'y_combined.npy'), allow_pickle=True)\n",
    "\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=wandb.config.seed, gamma=wandb.config.gamma, C=wandb.config.C)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "wandb.log({\"Train Accuracy\": svm.score(X_train_std, y_train), \n",
    "           \"Test Accuracy\": svm.score(X_test_std, y_test)})\n",
    "\n",
    "\n",
    "dump(svm, 'models/iris_model.joblib')\n",
    "\n",
    "artifact = wandb.Artifact('iris_model', type='model')\n",
    "artifact.add_file('models/iris_model.joblib')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
